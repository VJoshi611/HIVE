


Scenario Based questions:

1] Will the reducer work or not if you use “Limit 1” in any HiveQL query?
--> For Limit constraint reducer will not work. It works in scenario where queries include group by, joins, aggregate functions, or complex constraints. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2] Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
--> 
The default metastore configuration allows only one Hive session to be opened at a time for accessing the metastore. Therefore, if multiple clients try to access the metastore at the 
same time, they will get an error. One has to use a standalone metastore, i.e. Local or remote metastore configuration in Apache Hive for allowing access to multiple clients concurrently. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3] Suppose, I create a table that contains details of all the transactions done by the customers: 
CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. 
How will you solve this problem and list the steps that I will be taking in order to do so?
--> 
we can use partition method for optimization of queries.It will only fetch only partitoned data from tables. we can use below steps to perform partition in table :

1.Create a partitioned table, say partitioned_transaction:
CREATE TABLE partitioned_transaction (cust_id INT, amount FLOAT, country STRING) PARTITIONED BY (month STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; 

2. Enable dynamic partitioning in Hive:
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

3. Transfer the data from the non – partitioned table into the newly created partitioned table:
INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details;

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4] How can you add a new partition for the month December in the above partitioned table?
--> we can use alter command to make changes in query.
ALTER TABLE partitioned_transaction ADD PARTITION (month=’Dec’) LOCATION  ‘/partitioned_transaction’;

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5] I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least 
one static partition column. How will you remove this error?
--> By default hive set static partition in it. To allow dynamic partition we have to run below commands :
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

6] Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
--> 
Hive provides a specific SerDe for working with CSV files. We can use this SerDe for the sample.csv by issuing following commands:

CREATE EXTERNAL TABLE sample
(id int, first_name string, last_name string, email string, gender string, ip_address string) 
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
STORED AS TEXTFILE LOCATION ‘/temp’;

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

7] Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
-->
One can use the SequenceFile format which will group these small files together to form a single sequence file. The steps that will be followed in doing so are as follows:

1) Create a temporary table:
CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS TEXTFILE;

2) Load the data into temp_table:
LOAD DATA INPATH ‘/input’ INTO TABLE temp_table;

3) Create a table that will store data in SequenceFile format:
CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS SEQUENCEFILE;

4) Transfer the data from the temporary table into the sample_seqfile table:
INSERT OVERWRITE TABLE sample_seqfile SELECT * FROM temp_table;

Hence, a single SequenceFile is generated which contains the data present in all of the input files and therefore, the problem of having lots of small files is finally eliminated.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

8] LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
--> 
This problem occur because of overwrite parameter.
method :-  load data in hadoop file path. Once file in Hadoop file system, we can load the file into the table via load command in hive console
LOAD DATA LOCAL INPATH ‘Home/country/state/’
INTO TABLE address;

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

9] Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
--> Yes we can add 100 nodes when we already have 100 nodes in Hive. To execute this process we have to make some configuration changes in Hive.


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------




Hive Practical questions:

1] Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

1) SELECT * FROM CUSTOMERS c INNER JOIN ORDER o ON c.ID = o.ID;

2) SELECT * FROM CUSTOMERS c LEFT OUTER JOIN ORDER o ON c.ID = o.ID;

3) SELECT * FROM CUSTOMERS c RIGHT OUTER JOIN ORDER o ON c.ID = o.ID;

4) SELECT * FROM CUSTOMERS c FULL OUTER JOIN ORDER o ON c.ID = o.ID; 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
create table airquality
(
dates Date,
times string,
co float,
pt08c0 int,
nmhc int,
c6h6 float,
pt08nmhc int,
nox int,
pt08nox int,
no2 int,
pt08no2 int,
pt0803 int,
t float,
rh float,
ah float
)
row format delimited
fields terminated by ','
tblproperties("skip.header.line.count"="1");

2. try to place a data into table location
hadoop fs -put AirQualityUCI.csv /tmp
load data inpath '/tmp/AirQualityUCI.csv' into table airquality;

3. Perform a select operation . 
select * from airquality;

4. Fetch the result of the select operation in your local as a csv file . 
insert overwrite local directory '/download/output.csv' select max(co) from airquality;

5. Perform group by operation . 
select nmhc from airquality group by dates;

7. Perform filter operation at least 5 kinds of filter examples . 
1) select nmhc from airqulaity where date = '10-04-2004';
2) select co from airqulaity where t < 9.8 or t > 3.4;
3) select max(pt08co) from airquality group by dates;
4) select nmhc from airquality where rh > 80;
5) select c6h6 from airquality group by dates order by c6h6 limit 5;  

8. show and example of regex operation

9. alter table operation 
alter table airquality change times times Time;

10 . drop table operation
drop table airquality;

12 . order by operation . 
select c6h6 from airquality group by dates order by c6h6 limit 5;  

13 . where clause operations you have to perform . 
select nmhc from airqulaity where date = '10-04-2004';

14 . sorting operation you have to perform . 
select c6h6 from airquality group by dates order by c6h6 DESC limit 1; 

15 . distinct operation you have to perform . 
select distinct dates from airquality;

16 . like an operation you have to perform . 
select nmhc from airqulaity where date like "10-04-2004";

17 . union operation you have to perform . 
select nmhc from airqulaity where date = '10-04-2004'
unique
select nmhc from airqulaity where t < 9.8 or t > 3.4;


18 . table view operation you have to perform . 
create view air as
select nmhc from airquality where dates = "10-03-2005";

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.
fetch rows to python itself into a list of tuples and mimic the join or filter operations





























